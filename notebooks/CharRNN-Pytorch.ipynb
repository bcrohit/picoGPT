{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573cd6e3-f7b6-4f85-a2b9-1df4ffc32b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c56eeee-f681-4d4a-bb0d-9f10d95e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f3aa54-cbda-487b-85d2-87adb09a0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "shakespear_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "shakespear = requests.get(shakespear_url).text\n",
    "shakespear = shakespear.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2b5017-c31e-421c-bce5-7761e5a7d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 8\n",
    "vocab_size = len(set(shakespear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e469f10-8604-4e7b-99dd-d8925ba8a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char level tokenization\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.str_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.int_to_str = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "    def encode(self, text):            \n",
    "        ids = [self.str_to_int[char] for char in text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[num] for num in ids]\n",
    "        text = ''.join(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b23ec01-3441-4146-a6af-b7041a72afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PicoGPTDataset(Dataset):\n",
    "    def __init__(self, token_ids, context_size, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        for i in range(0, len(token_ids) - context_size, stride):\n",
    "            input_chunk = token_ids[i:i + context_size]\n",
    "            target_chunk = token_ids[i + 1: i + context_size + 1]\n",
    "            # requires_grad_(True) tells all of the input tensors \n",
    "            # should be used to calculate the gradients\n",
    "            # it is set to False here by default\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "826fd29f-7b61-43c3-b29e-84b120d3b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, batch_size=4, test_size=0.1, context_size=context_size, stride=1, shuffle=True, drop_last=True):\n",
    "    tokenizer = CharTokenizer(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "\n",
    "    test = int(test_size*len(token_ids))\n",
    "    test_tokens = token_ids[test:]\n",
    "    train_tokens = token_ids[:test]\n",
    "    dataset_train = PicoGPTDataset(train_tokens, context_size, stride)\n",
    "    dataset_test = PicoGPTDataset(test_tokens, context_size, stride)\n",
    "    \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dde7ff0-1230-48c2-8aa1-c1bea78027c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = create_dataloader(shakespear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620c0e24-f9b8-419d-a374-3afa4cf7f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33876134-5aaf-4cc0-b790-ec9514caa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, vocab_size, embed_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output, hidden = self.gru(embeds)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3be888d2-94bd-4c41-9e4b-281bd9eac395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(hidden_size=128, output_size=8, vocab_size=vocab_size, embed_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abd813e6-f325-4ecb-b8c8-01f58a1527d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85edbc-47ed-4ff0-824e-5a00b1aa7817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8f955-69f5-4964-b1f0-855849f21421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff922156-edb7-45c7-8074-d59446cb0ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f3e4a06-ee95-4394-ba7f-4136a865195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = 39  # Set accordingly\n",
    "embedding_dim = 16\n",
    "hidden_size = 128\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = ShakespeareModel(vocab_size, embedding_dim, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "checkpoint_path = \"my_shakespeare_model.pth\"\n",
    "i=0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i==10:\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    # Validation loop\n",
    "    # model.eval()\n",
    "    # correct, total = 0, 0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch in valid_set:  # Assume valid_set is a DataLoader\n",
    "    #         inputs, targets = batch\n",
    "    #         inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "    #         outputs = model(inputs)\n",
    "    #         predicted = torch.argmax(outputs, dim=-1)\n",
    "    #         correct += (predicted == targets).sum().item()\n",
    "    #         total += targets.numel()\n",
    "    \n",
    "    # val_acc = correct / total\n",
    "    # print(f\"Epoch {epoch+1}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # # Save the best model\n",
    "    # if val_acc > best_val_acc:\n",
    "    #     best_val_acc = val_acc\n",
    "    #     torch.save(model.state_dict(), checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd9ed6f5-1850-426f-88de-98210fb69b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 39])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1af931-e1b5-4559-9d64-4841284093b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_losses = train(model, dataloader, n_epoch=27, learning_rate=0.15, report_every=5)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00c69a06-46cf-492e-963f-e8aa1c40104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2246855-f6c4-4b36-bc3b-26369f4514dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18, 27, 30, 32, 20,  1, 32, 20],\n",
       "        [24, 16, 21, 31, 20,  1, 18, 30],\n",
       "        [ 0, 27, 18, 32, 17, 26, 17, 30],\n",
       "        [31,  8,  0, 15, 27, 33, 31, 21]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6f28b58-1347-4040-a421-8ddb5611fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d69e689-cab2-4407-93e3-9e9da404f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_generate(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "           \n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f1018471-6665-4929-bac3-8addf686f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.embeddings.weight.shape[0]\n",
    "    encoded = encoded = torch.tensor(tokenizer.encode(start_context)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        token_ids = simple_text_generate(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=7, context_size=context_size\n",
    "        )\n",
    "        decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6c36030-e324-413a-b050-4fb46c42916c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thy ,,' --\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(model, tokenizer, 'cpu', 'thy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d3a3849c-e0c8-47a5-9d63-29253982960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ac2d2a5e-cca6-4612-be84-9ca9552c8ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 27 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Work\\Projects\\picoGPT\\utils.py:37\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[1;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_batch)\n\u001b[1;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Work\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 27 is out of bounds."
     ]
    }
   ],
   "source": [
    "calc_loss_batch(inputs, targets, model, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb26ed95-7b01-451e-b470-4fb9250a2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "227b1824-30b0-4aa2-b131-689e2f41674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20057,   995, 18149]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tok = tiktoken.get_encoding('gpt2')\n",
    "text_to_token_ids('thy world nonsense', tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4f729-5d7f-489f-9825-6e69b033bf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
