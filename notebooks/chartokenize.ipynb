{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63062912-c17a-4cd7-aef2-0f5fdc275fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8dab1b-43ae-45d5-aea3-ddf15f57f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32a58ef-59c7-4a7c-9971-0e2cdbc912c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something to do with raw data\n",
    "with open(\"data\\\\names.txt\") as fp:\n",
    "    names = fp.read()\n",
    "\n",
    "pattern = r'[^a-z\\n]'\n",
    "names = re.sub('0', 'o', names)\n",
    "names = re.sub(pattern, '', names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6c115db-f15c-4516-b39f-202357e7bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# char level tokenization\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.str_to_int = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.int_to_str = { i:ch for i,ch in enumerate(self.chars) }\n",
    "\n",
    "    def encode(self, text):            \n",
    "        ids = [self.str_to_int[char] for char in text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[num] for num in ids]\n",
    "        text = ''.join(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131fa9f1-7a45-4925-a8c5-3f06414ea92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f12abc6-04ec-4a12-8f20-65bdc94d059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(names)\n",
    "ids = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c19549-fc35-455d-848a-3e2fa3d4f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = names.split(\"\\n\")\n",
    "avg_name_len = sum([len(i) for i in name_list]) / len(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9eb10a-6040-4d91-80c3-726877a6b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "stride = max_length  # or /2 for overlap\n",
    "samples = []\n",
    "for i in range(0, len(ids) - max_length, stride):\n",
    "    input_ids = ids[i:i+max_length]\n",
    "    target_ids = ids[i+1:i+max_length+1]\n",
    "    samples.append((input_ids, target_ids))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc05d9ce-df49-4e64-9656-ca8be1f42a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 1,  1,  2,  9,  4,  0,  1,  1,  2,  9,  4,  1,  0,  1,  1,  3,  8,  1,\n",
       "          12,  0,  1,  1,  4,  5, 19,  8,  0,  1,  1,  4,  9, 12]),\n",
       "  tensor([ 1,  2,  9,  4,  0,  1,  1,  2,  9,  4,  1,  0,  1,  1,  3,  8,  1, 12,\n",
       "           0,  1,  1,  4,  5, 19,  8,  0,  1,  1,  4,  9, 12,  0]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bec79c8-4834-4141-9425-c11d06664f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70ca8cc1-fdd6-44fa-bdfe-9168178c1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PicoGPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    " \n",
    "        token_ids = tokenizer.encode(txt)\n",
    " \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2678ebc3-229b-4b9e-932a-798cfb08bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = PicoGPTDataset(names, tokenizer, max_length, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0429058-8e65-41de-bae2-e44b12fe9c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naadish\\naaditya\\naaenab\\naafreen\\na'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(d.input_ids[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1c90e62-b753-4df1-8e13-6697e85f1769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aadish\\naaditya\\naaenab\\naafreen\\naa'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(d.target_ids[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5677a66-a75f-47a2-aaa2-e20fc07ca2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acea6b81-f902-4d38-aef9-06b5b6a192aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_size\": 27,\n",
    "    \"context_length\": 32,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be6f244f-ee11-4a15-8032-2cc00bd6b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picogpt import *\n",
    "torch.manual_seed(123)\n",
    "model = PicoGPTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "485f4c04-3b35-4fcc-be1c-7b827f1210c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [18, 15]\n",
      "encoded_tensor.shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"ro\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4646e63a-7e43-46ff-b48b-db2402bcea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1b147cc-3d10-43ac-82ab-deee47111728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[18, 15, 17, 20,  0, 22, 12, 16,  0,  0,  0, 22, 22,  0, 22, 22, 12, 16,\n",
      "          5,  1, 21,  2,  4, 10, 21,  4, 13,  7, 20,  1, 22, 18]])\n",
      "Output length: 32\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = simple_text_generate(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=30, \n",
    "    context_size=config[\"context_length\"]\n",
    " )\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f8366db-3e55-4e12-87c3-64691e6a1ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roqt\n",
      "vlp\n",
      "\n",
      "\n",
      "vv\n",
      "vvlpeaubdjudmgtavr\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64dcce-04bc-49f9-a8a9-d2e2d25d90f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddc9ab-e6d6-4fdf-8dcc-5785034974ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab350f78-ff23-44f4-bd3c-802147606f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d15ba3-4bd3-4c51-8ec5-1534b89b5de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6c63f-db42-4044-b4f9-6a6f3dbdea7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437c510-102a-4789-b125-c499136ca37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8343ff3-ef23-4e20-a897-0ca0fe7dc166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e13751-08d8-4406-bd9e-b65915a4b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32472f-c2ab-4972-952f-bfeb7c94d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cb4c7-ceab-4023-87ad-9bfb1edc2a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963b4e6-9d3f-4cd0-a38c-95871bf925a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdf66ca-7bea-4083-84e6-514441897b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99b210-81dd-4c4c-bcd9-b34ca39872cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
